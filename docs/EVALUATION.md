# Module d'√âvaluation et Reproductibilit√©

Ce module impl√©mente les m√©triques d'√©valuation demand√©es dans le point 4 du projet EDA.

## üìä M√©triques Impl√©ment√©es

### 1. Taux d'Acceptation (via feedback)
- **Calcul** : Proportion de motifs "lik√©s" par rapport au total des feedbacks
- **Formule** : `likes / (likes + dislikes + neutral)`
- **Interpr√©tation** : Un taux √©lev√© (>70%) indique que les motifs sont pertinents

### 2. Diversit√©
- **Calcul** : Distance de Jaccard moyenne entre tous les paires de motifs
- **Formule** : `1 - (intersection / union)` pour chaque paire
- **Interpr√©tation** : Plus c'est √©lev√© (>0.7), plus les motifs sont diff√©rents

### 3. Couverture (Coverage)
Trois aspects de couverture sont mesur√©s :
- **Couverture motifs** : `nb_motifs_√©chantillonn√©s / nb_motifs_total`
- **Couverture items** : `nb_items_uniques_√©chantillon / nb_items_uniques_total`
- **Couverture support** : `somme_supports_√©chantillon / somme_supports_total`

### 4. Stabilit√© (Sensibilit√© √† la seed)
- **Calcul** : Similarit√© de Jaccard moyenne entre √©chantillons avec diff√©rentes seeds
- **M√©thode** : 10 √©chantillonnages avec seeds diff√©rentes (42, 43, 44, ...)
- **Interpr√©tation** : Plus c'est √©lev√© (>0.7), plus l'algorithme est stable

### 5. Temps de R√©ponse
- **Mesures** : Temps moyen, min, max, √©cart-type sur 5 ex√©cutions
- **Objectif** : < 2-3 secondes pour une exp√©rience interactive
- **Interpr√©tation** : Performance critique pour l'UX

## üèóÔ∏è Architecture

### Backend

#### `backend/core/evaluation.py`
Classe `PatternEvaluator` avec m√©thodes :
- `calculate_acceptance_rate(feedback_list)` : Taux d'acceptation
- `calculate_diversity(patterns_df)` : Diversit√© des motifs
- `calculate_coverage(sampled, all_patterns)` : Couverture
- `calculate_stability(func, patterns, params)` : Stabilit√©
- `measure_response_time(func, patterns, params)` : Performance
- `comprehensive_evaluation(...)` : √âvaluation compl√®te

#### `backend/core/sampling.py`
Modifications :
- Ajout de `feedback_history` pour tracker les feedbacks
- Mise √† jour de `user_feedback()` pour enregistrer l'historique

#### `backend/api/routes.py`
Nouveau endpoint :
- `GET /api/patterns/evaluate` : Retourne toutes les m√©triques d'√©valuation

### Frontend

#### `frontend/components/evaluation.py`
Composants de visualisation :
- `display_evaluation_metrics(data)` : Affichage complet avec graphiques
- `display_evaluation_summary(data)` : R√©sum√© compact

#### `frontend/app.py`
- Tab 4 "üìä Analyse" transform√© en "üìä √âvaluation & Reproductibilit√©"
- Int√©gration des composants d'√©valuation
- Boutons d'export CSV et JSON

## üìà Utilisation

### Via l'Interface Web

1. **Charger un dataset** (Tab "Upload")
2. **Extraire les motifs** (Tab "Motifs")
3. **Donner des feedbacks** sur les motifs √©chantillonn√©s (üëç/üëé)
4. **Lancer l'√©valuation** (Tab "√âvaluation")
   - Cliquer sur "üöÄ √âvaluer"
   - Consulter les m√©triques dans les 5 onglets
   - Exporter les r√©sultats en CSV ou JSON

### Via l'API

```python
import requests

# Lancer l'√©valuation
response = requests.get("http://backend:8000/api/patterns/evaluate")
evaluation = response.json()

print(f"Score global: {evaluation['evaluation']['overall_score']:.2%}")
print(f"Taux d'acceptation: {evaluation['evaluation']['acceptance']['acceptance_rate']:.2%}")
print(f"Diversit√©: {evaluation['evaluation']['diversity']['diversity_score']:.3f}")
print(f"Couverture: {evaluation['evaluation']['coverage']['pattern_coverage']:.2%}")
print(f"Stabilit√©: {evaluation['evaluation']['stability']['stability_score']:.3f}")
print(f"Temps moyen: {evaluation['evaluation']['response_time']['mean_time']:.3f}s")
```

## üéØ Score Global

Le score global est une moyenne pond√©r√©e des m√©triques :
```
Score = 0.30 √ó Acceptation + 0.25 √ó Diversit√© + 0.25 √ó Couverture + 0.20 √ó Stabilit√©
```

## üìä Visualisations

L'interface propose plusieurs types de visualisations :
- **Jauge** : Score global et diversit√©
- **Barres** : Distribution des feedbacks, couverture
- **Histogramme** : Distribution des similarit√©s (stabilit√©)
- **Indicateurs** : Temps de r√©ponse

## üîÑ Reproductibilit√©

### Stabilit√© de l'√©chantillonnage
Le module teste la reproductibilit√© en :
1. Ex√©cutant l'algorithme 10 fois avec des seeds diff√©rentes
2. Calculant la similarit√© de Jaccard entre chaque paire d'√©chantillons
3. Moyennant ces similarit√©s pour obtenir le score de stabilit√©

### Export des r√©sultats
Deux formats d'export :
- **CSV** : M√©triques principales pour analyse dans Excel/R/Python
- **JSON** : Rapport complet avec tous les d√©tails

## ‚ö° Performance

### Optimisations impl√©ment√©es
- Vectorisation numpy pour calculs de diversit√© et redondance
- Limitation des comparaisons pour la redondance (patterns de taille similaire)
- Cache des r√©sultats interm√©diaires

### Temps de r√©ponse typiques
- Dataset < 1000 motifs : < 1s
- Dataset 1000-5000 motifs : 1-3s
- Dataset > 5000 motifs : 3-10s

## üß™ Exemple de R√©sultat

```json
{
  "evaluation": {
    "acceptance": {
      "acceptance_rate": 0.75,
      "total_feedbacks": 20,
      "likes": 15,
      "dislikes": 3,
      "neutral": 2
    },
    "diversity": {
      "diversity_score": 0.682,
      "unique_items_count": 45,
      "average_pattern_length": 3.2
    },
    "coverage": {
      "pattern_coverage": 0.10,
      "item_coverage": 0.85,
      "support_coverage": 0.62
    },
    "stability": {
      "stability_score": 0.721,
      "mean_similarity": 0.721,
      "std_similarity": 0.089
    },
    "response_time": {
      "mean_time": 1.234,
      "std_time": 0.056,
      "min_time": 1.189,
      "max_time": 1.312
    },
    "overall_score": 0.698
  }
}
```

## üìù Notes d'Impl√©mentation

### Choix de design
1. **S√©paration des responsabilit√©s** : √âvaluation dans module d√©di√©
2. **Flexibilit√©** : M√©thodes individuelles + √©valuation compl√®te
3. **Visualisation** : Graphiques interactifs Plotly
4. **Export** : Formats multiples pour diff√©rents usages

### Limitations connues
1. Stabilit√© : Limit√©e √† 10 it√©rations pour performance
2. Temps de r√©ponse : Mesur√© sur 5 runs (peut varier selon charge syst√®me)
3. Acceptance rate : N√©cessite des feedbacks utilisateur

### Extensions possibles
- Sauvegarder l'historique des √©valuations
- Comparer diff√©rentes strat√©gies d'√©chantillonnage
- Tests statistiques (t-test) pour la stabilit√©
- M√©triques additionnelles (nouveaut√©, surprise globale)

## üîó R√©f√©rences

- **Diversit√©** : Jaccard Distance pour similarit√© de sets
- **Stabilit√©** : Approche Monte Carlo avec seeds multiples
- **Performance** : Benchmarking avec r√©p√©titions

## üë• Auteurs

Module d'√©valuation impl√©ment√© pour le projet EDA - SCIA-G
